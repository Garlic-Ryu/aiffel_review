{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b661ec2d",
   "metadata": {},
   "source": [
    "## (1) 데이터 가져오기\n",
    "sklearn.datasets 의 load_diabetes 에서 데이터를 가져와주세요.\n",
    "diabetes 의 data 를 df_X 에, target 을 df_y 에 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bad4d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "dataset = sklearn.datasets.load_diabetes()\n",
    "\n",
    "df_x=dataset.data\n",
    "df_y=dataset.target\n",
    "\n",
    "print(df_x.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55619c4",
   "metadata": {},
   "source": [
    "## (2) 모델에 입력할 데이터 X 준비하기\n",
    "df_X 에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd308b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_X = np.array(df_x)\n",
    "print(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e5686",
   "metadata": {},
   "source": [
    "## (3) 모델에 예측할 데이터 y 준비하기\n",
    "df_y 에 있는 값들을 numpy array로 변환해서 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81a9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "df_y = np.array(df_y)\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891d0af",
   "metadata": {},
   "source": [
    "## (4) train 데이터와 test 데이터로 분리하기\n",
    "X 와 y  데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e3a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f8a39",
   "metadata": {},
   "source": [
    "## (5) 모델 준비하기\n",
    "입력 데이터 개수에 맞는 가중치 W 와 b 를 준비해주세요.\n",
    "모델 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1551b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d011fe8",
   "metadata": {},
   "source": [
    "## (6) 손실함수 loss 정의하기\n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d74c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "  mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "  return mse\n",
    "\n",
    "def loss(x, w, b, y):\n",
    "  predictions = model(x, w, b)\n",
    "  L = MSE(predictions, y)\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e792f",
   "metadata": {},
   "source": [
    "## (7) 기울기를 구하는 gradient 함수 구현하기\n",
    "기울기를 계산하는 gradient  함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b03f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290176c",
   "metadata": {},
   "source": [
    "## (8) 하이퍼 파라미터인 학습률 설정하기\n",
    "학습률, learning rate 를 설정해주세요\n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99160a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61043c",
   "metadata": {},
   "source": [
    "## (9) 모델 학습하기\n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "입력하는 데이터인 X 에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c55462",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/4237560060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_ = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f53c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 6322.4011\n",
      "Iteration 20 : Loss 5978.0664\n",
      "Iteration 30 : Loss 5901.1278\n",
      "Iteration 40 : Loss 5829.5389\n",
      "Iteration 50 : Loss 5760.2017\n",
      "Iteration 60 : Loss 5693.0057\n",
      "Iteration 70 : Loss 5627.8776\n",
      "Iteration 80 : Loss 5564.7471\n",
      "Iteration 90 : Loss 5503.5463\n",
      "Iteration 100 : Loss 5444.2097\n",
      "Iteration 110 : Loss 5386.6741\n",
      "Iteration 120 : Loss 5330.8785\n",
      "Iteration 130 : Loss 5276.7642\n",
      "Iteration 140 : Loss 5224.2743\n",
      "Iteration 150 : Loss 5173.3541\n",
      "Iteration 160 : Loss 5123.9507\n",
      "Iteration 170 : Loss 5076.0131\n",
      "Iteration 180 : Loss 5029.4922\n",
      "Iteration 190 : Loss 4984.3404\n",
      "Iteration 200 : Loss 4940.5118\n",
      "Iteration 210 : Loss 4897.9624\n",
      "Iteration 220 : Loss 4856.6493\n",
      "Iteration 230 : Loss 4816.5314\n",
      "Iteration 240 : Loss 4777.5690\n",
      "Iteration 250 : Loss 4739.7237\n",
      "Iteration 260 : Loss 4702.9585\n",
      "Iteration 270 : Loss 4667.2376\n",
      "Iteration 280 : Loss 4632.5265\n",
      "Iteration 290 : Loss 4598.7920\n",
      "Iteration 300 : Loss 4566.0020\n",
      "Iteration 310 : Loss 4534.1254\n",
      "Iteration 320 : Loss 4503.1323\n",
      "Iteration 330 : Loss 4472.9938\n",
      "Iteration 340 : Loss 4443.6821\n",
      "Iteration 350 : Loss 4415.1701\n",
      "Iteration 360 : Loss 4387.4319\n",
      "Iteration 370 : Loss 4360.4425\n",
      "Iteration 380 : Loss 4334.1776\n",
      "Iteration 390 : Loss 4308.6138\n",
      "Iteration 400 : Loss 4283.7285\n",
      "Iteration 410 : Loss 4259.5000\n",
      "Iteration 420 : Loss 4235.9071\n",
      "Iteration 430 : Loss 4212.9297\n",
      "Iteration 440 : Loss 4190.5481\n",
      "Iteration 450 : Loss 4168.7432\n",
      "Iteration 460 : Loss 4147.4970\n",
      "Iteration 470 : Loss 4126.7915\n",
      "Iteration 480 : Loss 4106.6099\n",
      "Iteration 490 : Loss 4086.9357\n",
      "Iteration 500 : Loss 4067.7529\n",
      "Iteration 510 : Loss 4049.0461\n",
      "Iteration 520 : Loss 4030.8006\n",
      "Iteration 530 : Loss 4013.0020\n",
      "Iteration 540 : Loss 3995.6364\n",
      "Iteration 550 : Loss 3978.6905\n",
      "Iteration 560 : Loss 3962.1514\n",
      "Iteration 570 : Loss 3946.0065\n",
      "Iteration 580 : Loss 3930.2439\n",
      "Iteration 590 : Loss 3914.8519\n",
      "Iteration 600 : Loss 3899.8193\n",
      "Iteration 610 : Loss 3885.1350\n",
      "Iteration 620 : Loss 3870.7888\n",
      "Iteration 630 : Loss 3856.7704\n",
      "Iteration 640 : Loss 3843.0699\n",
      "Iteration 650 : Loss 3829.6780\n",
      "Iteration 660 : Loss 3816.5854\n",
      "Iteration 670 : Loss 3803.7834\n",
      "Iteration 680 : Loss 3791.2633\n",
      "Iteration 690 : Loss 3779.0169\n",
      "Iteration 700 : Loss 3767.0362\n",
      "Iteration 710 : Loss 3755.3135\n",
      "Iteration 720 : Loss 3743.8412\n",
      "Iteration 730 : Loss 3732.6122\n",
      "Iteration 740 : Loss 3721.6195\n",
      "Iteration 750 : Loss 3710.8564\n",
      "Iteration 760 : Loss 3700.3162\n",
      "Iteration 770 : Loss 3689.9927\n",
      "Iteration 780 : Loss 3679.8799\n",
      "Iteration 790 : Loss 3669.9717\n",
      "Iteration 800 : Loss 3660.2624\n",
      "Iteration 810 : Loss 3650.7467\n",
      "Iteration 820 : Loss 3641.4190\n",
      "Iteration 830 : Loss 3632.2742\n",
      "Iteration 840 : Loss 3623.3074\n",
      "Iteration 850 : Loss 3614.5137\n",
      "Iteration 860 : Loss 3605.8884\n",
      "Iteration 870 : Loss 3597.4269\n",
      "Iteration 880 : Loss 3589.1250\n",
      "Iteration 890 : Loss 3580.9782\n",
      "Iteration 900 : Loss 3572.9827\n",
      "Iteration 910 : Loss 3565.1342\n",
      "Iteration 920 : Loss 3557.4291\n",
      "Iteration 930 : Loss 3549.8636\n",
      "Iteration 940 : Loss 3542.4340\n",
      "Iteration 950 : Loss 3535.1370\n",
      "Iteration 960 : Loss 3527.9690\n",
      "Iteration 970 : Loss 3520.9268\n",
      "Iteration 980 : Loss 3514.0073\n",
      "Iteration 990 : Loss 3507.2073\n",
      "Iteration 1000 : Loss 3500.5239\n",
      "Iteration 1010 : Loss 3493.9542\n",
      "Iteration 1020 : Loss 3487.4954\n",
      "Iteration 1030 : Loss 3481.1448\n",
      "Iteration 1040 : Loss 3474.8998\n",
      "Iteration 1050 : Loss 3468.7578\n",
      "Iteration 1060 : Loss 3462.7163\n",
      "Iteration 1070 : Loss 3456.7730\n",
      "Iteration 1080 : Loss 3450.9255\n",
      "Iteration 1090 : Loss 3445.1715\n",
      "Iteration 1100 : Loss 3439.5090\n",
      "Iteration 1110 : Loss 3433.9358\n",
      "Iteration 1120 : Loss 3428.4498\n",
      "Iteration 1130 : Loss 3423.0491\n",
      "Iteration 1140 : Loss 3417.7316\n",
      "Iteration 1150 : Loss 3412.4956\n",
      "Iteration 1160 : Loss 3407.3392\n",
      "Iteration 1170 : Loss 3402.2606\n",
      "Iteration 1180 : Loss 3397.2582\n",
      "Iteration 1190 : Loss 3392.3303\n",
      "Iteration 1200 : Loss 3387.4752\n",
      "Iteration 1210 : Loss 3382.6913\n",
      "Iteration 1220 : Loss 3377.9773\n",
      "Iteration 1230 : Loss 3373.3315\n",
      "Iteration 1240 : Loss 3368.7526\n",
      "Iteration 1250 : Loss 3364.2391\n",
      "Iteration 1260 : Loss 3359.7896\n",
      "Iteration 1270 : Loss 3355.4030\n",
      "Iteration 1280 : Loss 3351.0778\n",
      "Iteration 1290 : Loss 3346.8129\n",
      "Iteration 1300 : Loss 3342.6070\n",
      "Iteration 1310 : Loss 3338.4589\n",
      "Iteration 1320 : Loss 3334.3676\n",
      "Iteration 1330 : Loss 3330.3318\n",
      "Iteration 1340 : Loss 3326.3505\n",
      "Iteration 1350 : Loss 3322.4226\n",
      "Iteration 1360 : Loss 3318.5472\n",
      "Iteration 1370 : Loss 3314.7232\n",
      "Iteration 1380 : Loss 3310.9497\n",
      "Iteration 1390 : Loss 3307.2256\n",
      "Iteration 1400 : Loss 3303.5501\n",
      "Iteration 1410 : Loss 3299.9223\n",
      "Iteration 1420 : Loss 3296.3413\n",
      "Iteration 1430 : Loss 3292.8062\n",
      "Iteration 1440 : Loss 3289.3162\n",
      "Iteration 1450 : Loss 3285.8706\n",
      "Iteration 1460 : Loss 3282.4685\n",
      "Iteration 1470 : Loss 3279.1091\n",
      "Iteration 1480 : Loss 3275.7918\n",
      "Iteration 1490 : Loss 3272.5157\n",
      "Iteration 1500 : Loss 3269.2801\n",
      "Iteration 1510 : Loss 3266.0845\n",
      "Iteration 1520 : Loss 3262.9280\n",
      "Iteration 1530 : Loss 3259.8100\n",
      "Iteration 1540 : Loss 3256.7299\n",
      "Iteration 1550 : Loss 3253.6871\n",
      "Iteration 1560 : Loss 3250.6809\n",
      "Iteration 1570 : Loss 3247.7107\n",
      "Iteration 1580 : Loss 3244.7760\n",
      "Iteration 1590 : Loss 3241.8762\n",
      "Iteration 1600 : Loss 3239.0107\n",
      "Iteration 1610 : Loss 3236.1789\n",
      "Iteration 1620 : Loss 3233.3804\n",
      "Iteration 1630 : Loss 3230.6146\n",
      "Iteration 1640 : Loss 3227.8809\n",
      "Iteration 1650 : Loss 3225.1790\n",
      "Iteration 1660 : Loss 3222.5083\n",
      "Iteration 1670 : Loss 3219.8684\n",
      "Iteration 1680 : Loss 3217.2587\n",
      "Iteration 1690 : Loss 3214.6788\n",
      "Iteration 1700 : Loss 3212.1284\n",
      "Iteration 1710 : Loss 3209.6068\n",
      "Iteration 1720 : Loss 3207.1137\n",
      "Iteration 1730 : Loss 3204.6488\n",
      "Iteration 1740 : Loss 3202.2115\n",
      "Iteration 1750 : Loss 3199.8015\n",
      "Iteration 1760 : Loss 3197.4183\n",
      "Iteration 1770 : Loss 3195.0617\n",
      "Iteration 1780 : Loss 3192.7312\n",
      "Iteration 1790 : Loss 3190.4264\n",
      "Iteration 1800 : Loss 3188.1471\n",
      "Iteration 1810 : Loss 3185.8927\n",
      "Iteration 1820 : Loss 3183.6631\n",
      "Iteration 1830 : Loss 3181.4578\n",
      "Iteration 1840 : Loss 3179.2765\n",
      "Iteration 1850 : Loss 3177.1189\n",
      "Iteration 1860 : Loss 3174.9847\n",
      "Iteration 1870 : Loss 3172.8735\n",
      "Iteration 1880 : Loss 3170.7850\n",
      "Iteration 1890 : Loss 3168.7190\n",
      "Iteration 1900 : Loss 3166.6751\n",
      "Iteration 1910 : Loss 3164.6531\n",
      "Iteration 1920 : Loss 3162.6526\n",
      "Iteration 1930 : Loss 3160.6734\n",
      "Iteration 1940 : Loss 3158.7152\n",
      "Iteration 1950 : Loss 3156.7778\n",
      "Iteration 1960 : Loss 3154.8608\n",
      "Iteration 1970 : Loss 3152.9640\n",
      "Iteration 1980 : Loss 3151.0871\n",
      "Iteration 1990 : Loss 3149.2299\n",
      "Iteration 2000 : Loss 3147.3922\n",
      "Iteration 2010 : Loss 3145.5736\n",
      "Iteration 2020 : Loss 3143.7740\n",
      "Iteration 2030 : Loss 3141.9931\n",
      "Iteration 2040 : Loss 3140.2307\n",
      "Iteration 2050 : Loss 3138.4866\n",
      "Iteration 2060 : Loss 3136.7605\n",
      "Iteration 2070 : Loss 3135.0522\n",
      "Iteration 2080 : Loss 3133.3615\n",
      "Iteration 2090 : Loss 3131.6882\n",
      "Iteration 2100 : Loss 3130.0321\n",
      "Iteration 2110 : Loss 3128.3929\n",
      "Iteration 2120 : Loss 3126.7704\n",
      "Iteration 2130 : Loss 3125.1646\n",
      "Iteration 2140 : Loss 3123.5751\n",
      "Iteration 2150 : Loss 3122.0017\n",
      "Iteration 2160 : Loss 3120.4444\n",
      "Iteration 2170 : Loss 3118.9028\n",
      "Iteration 2180 : Loss 3117.3768\n",
      "Iteration 2190 : Loss 3115.8663\n",
      "Iteration 2200 : Loss 3114.3710\n",
      "Iteration 2210 : Loss 3112.8907\n",
      "Iteration 2220 : Loss 3111.4254\n",
      "Iteration 2230 : Loss 3109.9747\n",
      "Iteration 2240 : Loss 3108.5386\n",
      "Iteration 2250 : Loss 3107.1169\n",
      "Iteration 2260 : Loss 3105.7094\n",
      "Iteration 2270 : Loss 3104.3159\n",
      "Iteration 2280 : Loss 3102.9364\n",
      "Iteration 2290 : Loss 3101.5705\n",
      "Iteration 2300 : Loss 3100.2183\n",
      "Iteration 2310 : Loss 3098.8795\n",
      "Iteration 2320 : Loss 3097.5539\n",
      "Iteration 2330 : Loss 3096.2415\n",
      "Iteration 2340 : Loss 3094.9421\n",
      "Iteration 2350 : Loss 3093.6555\n",
      "Iteration 2360 : Loss 3092.3816\n",
      "Iteration 2370 : Loss 3091.1202\n",
      "Iteration 2380 : Loss 3089.8713\n",
      "Iteration 2390 : Loss 3088.6346\n",
      "Iteration 2400 : Loss 3087.4101\n",
      "Iteration 2410 : Loss 3086.1976\n",
      "Iteration 2420 : Loss 3084.9969\n",
      "Iteration 2430 : Loss 3083.8080\n",
      "Iteration 2440 : Loss 3082.6307\n",
      "Iteration 2450 : Loss 3081.4649\n",
      "Iteration 2460 : Loss 3080.3105\n",
      "Iteration 2470 : Loss 3079.1673\n",
      "Iteration 2480 : Loss 3078.0353\n",
      "Iteration 2490 : Loss 3076.9142\n",
      "Iteration 2500 : Loss 3075.8040\n",
      "Iteration 2510 : Loss 3074.7046\n",
      "Iteration 2520 : Loss 3073.6158\n",
      "Iteration 2530 : Loss 3072.5376\n",
      "Iteration 2540 : Loss 3071.4698\n",
      "Iteration 2550 : Loss 3070.4123\n",
      "Iteration 2560 : Loss 3069.3650\n",
      "Iteration 2570 : Loss 3068.3278\n",
      "Iteration 2580 : Loss 3067.3006\n",
      "Iteration 2590 : Loss 3066.2833\n",
      "Iteration 2600 : Loss 3065.2757\n",
      "Iteration 2610 : Loss 3064.2778\n",
      "Iteration 2620 : Loss 3063.2895\n",
      "Iteration 2630 : Loss 3062.3107\n",
      "Iteration 2640 : Loss 3061.3412\n",
      "Iteration 2650 : Loss 3060.3810\n",
      "Iteration 2660 : Loss 3059.4300\n",
      "Iteration 2670 : Loss 3058.4880\n",
      "Iteration 2680 : Loss 3057.5551\n",
      "Iteration 2690 : Loss 3056.6310\n",
      "Iteration 2700 : Loss 3055.7157\n",
      "Iteration 2710 : Loss 3054.8092\n",
      "Iteration 2720 : Loss 3053.9112\n",
      "Iteration 2730 : Loss 3053.0218\n",
      "Iteration 2740 : Loss 3052.1408\n",
      "Iteration 2750 : Loss 3051.2682\n",
      "Iteration 2760 : Loss 3050.4038\n",
      "Iteration 2770 : Loss 3049.5476\n",
      "Iteration 2780 : Loss 3048.6995\n",
      "Iteration 2790 : Loss 3047.8595\n",
      "Iteration 2800 : Loss 3047.0273\n",
      "Iteration 2810 : Loss 3046.2030\n",
      "Iteration 2820 : Loss 3045.3865\n",
      "Iteration 2830 : Loss 3044.5776\n",
      "Iteration 2840 : Loss 3043.7764\n",
      "Iteration 2850 : Loss 3042.9827\n",
      "Iteration 2860 : Loss 3042.1965\n",
      "Iteration 2870 : Loss 3041.4176\n",
      "Iteration 2880 : Loss 3040.6461\n",
      "Iteration 2890 : Loss 3039.8817\n",
      "Iteration 2900 : Loss 3039.1246\n",
      "Iteration 2910 : Loss 3038.3745\n",
      "Iteration 2920 : Loss 3037.6314\n",
      "Iteration 2930 : Loss 3036.8953\n",
      "Iteration 2940 : Loss 3036.1660\n",
      "Iteration 2950 : Loss 3035.4436\n",
      "Iteration 2960 : Loss 3034.7278\n",
      "Iteration 2970 : Loss 3034.0188\n",
      "Iteration 2980 : Loss 3033.3163\n",
      "Iteration 2990 : Loss 3032.6204\n",
      "Iteration 3000 : Loss 3031.9310\n",
      "Iteration 3010 : Loss 3031.2479\n",
      "Iteration 3020 : Loss 3030.5712\n",
      "Iteration 3030 : Loss 3029.9007\n",
      "Iteration 3040 : Loss 3029.2365\n",
      "Iteration 3050 : Loss 3028.5784\n",
      "Iteration 3060 : Loss 3027.9264\n",
      "Iteration 3070 : Loss 3027.2805\n",
      "Iteration 3080 : Loss 3026.6405\n",
      "Iteration 3090 : Loss 3026.0064\n",
      "Iteration 3100 : Loss 3025.3781\n",
      "Iteration 3110 : Loss 3024.7557\n",
      "Iteration 3120 : Loss 3024.1390\n",
      "Iteration 3130 : Loss 3023.5279\n",
      "Iteration 3140 : Loss 3022.9225\n",
      "Iteration 3150 : Loss 3022.3226\n",
      "Iteration 3160 : Loss 3021.7283\n",
      "Iteration 3170 : Loss 3021.1394\n",
      "Iteration 3180 : Loss 3020.5559\n",
      "Iteration 3190 : Loss 3019.9778\n",
      "Iteration 3200 : Loss 3019.4049\n",
      "Iteration 3210 : Loss 3018.8373\n",
      "Iteration 3220 : Loss 3018.2749\n",
      "Iteration 3230 : Loss 3017.7176\n",
      "Iteration 3240 : Loss 3017.1654\n",
      "Iteration 3250 : Loss 3016.6182\n",
      "Iteration 3260 : Loss 3016.0761\n",
      "Iteration 3270 : Loss 3015.5389\n",
      "Iteration 3280 : Loss 3015.0065\n",
      "Iteration 3290 : Loss 3014.4790\n",
      "Iteration 3300 : Loss 3013.9563\n",
      "Iteration 3310 : Loss 3013.4384\n",
      "Iteration 3320 : Loss 3012.9252\n",
      "Iteration 3330 : Loss 3012.4166\n",
      "Iteration 3340 : Loss 3011.9126\n",
      "Iteration 3350 : Loss 3011.4132\n",
      "Iteration 3360 : Loss 3010.9183\n",
      "Iteration 3370 : Loss 3010.4279\n",
      "Iteration 3380 : Loss 3009.9419\n",
      "Iteration 3390 : Loss 3009.4604\n",
      "Iteration 3400 : Loss 3008.9831\n",
      "Iteration 3410 : Loss 3008.5102\n",
      "Iteration 3420 : Loss 3008.0415\n",
      "Iteration 3430 : Loss 3007.5771\n",
      "Iteration 3440 : Loss 3007.1168\n",
      "Iteration 3450 : Loss 3006.6607\n",
      "Iteration 3460 : Loss 3006.2087\n",
      "Iteration 3470 : Loss 3005.7607\n",
      "Iteration 3480 : Loss 3005.3168\n",
      "Iteration 3490 : Loss 3004.8769\n",
      "Iteration 3500 : Loss 3004.4409\n",
      "Iteration 3510 : Loss 3004.0088\n",
      "Iteration 3520 : Loss 3003.5805\n",
      "Iteration 3530 : Loss 3003.1561\n",
      "Iteration 3540 : Loss 3002.7355\n",
      "Iteration 3550 : Loss 3002.3187\n",
      "Iteration 3560 : Loss 3001.9056\n",
      "Iteration 3570 : Loss 3001.4961\n",
      "Iteration 3580 : Loss 3001.0903\n",
      "Iteration 3590 : Loss 3000.6882\n",
      "Iteration 3600 : Loss 3000.2896\n",
      "Iteration 3610 : Loss 2999.8945\n",
      "Iteration 3620 : Loss 2999.5030\n",
      "Iteration 3630 : Loss 2999.1150\n",
      "Iteration 3640 : Loss 2998.7303\n",
      "Iteration 3650 : Loss 2998.3491\n",
      "Iteration 3660 : Loss 2997.9713\n",
      "Iteration 3670 : Loss 2997.5969\n",
      "Iteration 3680 : Loss 2997.2257\n",
      "Iteration 3690 : Loss 2996.8578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3700 : Loss 2996.4932\n",
      "Iteration 3710 : Loss 2996.1318\n",
      "Iteration 3720 : Loss 2995.7736\n",
      "Iteration 3730 : Loss 2995.4186\n",
      "Iteration 3740 : Loss 2995.0667\n",
      "Iteration 3750 : Loss 2994.7179\n",
      "Iteration 3760 : Loss 2994.3721\n",
      "Iteration 3770 : Loss 2994.0294\n",
      "Iteration 3780 : Loss 2993.6897\n",
      "Iteration 3790 : Loss 2993.3530\n",
      "Iteration 3800 : Loss 2993.0193\n",
      "Iteration 3810 : Loss 2992.6885\n",
      "Iteration 3820 : Loss 2992.3606\n",
      "Iteration 3830 : Loss 2992.0355\n",
      "Iteration 3840 : Loss 2991.7133\n",
      "Iteration 3850 : Loss 2991.3939\n",
      "Iteration 3860 : Loss 2991.0774\n",
      "Iteration 3870 : Loss 2990.7635\n",
      "Iteration 3880 : Loss 2990.4525\n",
      "Iteration 3890 : Loss 2990.1441\n",
      "Iteration 3900 : Loss 2989.8384\n",
      "Iteration 3910 : Loss 2989.5354\n",
      "Iteration 3920 : Loss 2989.2350\n",
      "Iteration 3930 : Loss 2988.9373\n",
      "Iteration 3940 : Loss 2988.6421\n",
      "Iteration 3950 : Loss 2988.3495\n",
      "Iteration 3960 : Loss 2988.0594\n",
      "Iteration 3970 : Loss 2987.7718\n",
      "Iteration 3980 : Loss 2987.4868\n",
      "Iteration 3990 : Loss 2987.2042\n",
      "Iteration 4000 : Loss 2986.9240\n",
      "Iteration 4010 : Loss 2986.6463\n",
      "Iteration 4020 : Loss 2986.3710\n",
      "Iteration 4030 : Loss 2986.0980\n",
      "Iteration 4040 : Loss 2985.8274\n",
      "Iteration 4050 : Loss 2985.5592\n",
      "Iteration 4060 : Loss 2985.2932\n",
      "Iteration 4070 : Loss 2985.0296\n",
      "Iteration 4080 : Loss 2984.7682\n",
      "Iteration 4090 : Loss 2984.5090\n",
      "Iteration 4100 : Loss 2984.2521\n",
      "Iteration 4110 : Loss 2983.9974\n",
      "Iteration 4120 : Loss 2983.7449\n",
      "Iteration 4130 : Loss 2983.4945\n",
      "Iteration 4140 : Loss 2983.2463\n",
      "Iteration 4150 : Loss 2983.0002\n",
      "Iteration 4160 : Loss 2982.7563\n",
      "Iteration 4170 : Loss 2982.5144\n",
      "Iteration 4180 : Loss 2982.2745\n",
      "Iteration 4190 : Loss 2982.0368\n",
      "Iteration 4200 : Loss 2981.8010\n",
      "Iteration 4210 : Loss 2981.5673\n",
      "Iteration 4220 : Loss 2981.3355\n",
      "Iteration 4230 : Loss 2981.1057\n",
      "Iteration 4240 : Loss 2980.8779\n",
      "Iteration 4250 : Loss 2980.6520\n",
      "Iteration 4260 : Loss 2980.4281\n",
      "Iteration 4270 : Loss 2980.2060\n",
      "Iteration 4280 : Loss 2979.9858\n",
      "Iteration 4290 : Loss 2979.7675\n",
      "Iteration 4300 : Loss 2979.5510\n",
      "Iteration 4310 : Loss 2979.3364\n",
      "Iteration 4320 : Loss 2979.1235\n",
      "Iteration 4330 : Loss 2978.9125\n",
      "Iteration 4340 : Loss 2978.7033\n",
      "Iteration 4350 : Loss 2978.4958\n",
      "Iteration 4360 : Loss 2978.2901\n",
      "Iteration 4370 : Loss 2978.0861\n",
      "Iteration 4380 : Loss 2977.8838\n",
      "Iteration 4390 : Loss 2977.6832\n",
      "Iteration 4400 : Loss 2977.4843\n",
      "Iteration 4410 : Loss 2977.2871\n",
      "Iteration 4420 : Loss 2977.0915\n",
      "Iteration 4430 : Loss 2976.8976\n",
      "Iteration 4440 : Loss 2976.7053\n",
      "Iteration 4450 : Loss 2976.5146\n",
      "Iteration 4460 : Loss 2976.3255\n",
      "Iteration 4470 : Loss 2976.1380\n",
      "Iteration 4480 : Loss 2975.9520\n",
      "Iteration 4490 : Loss 2975.7676\n",
      "Iteration 4500 : Loss 2975.5847\n",
      "Iteration 4510 : Loss 2975.4034\n",
      "Iteration 4520 : Loss 2975.2236\n",
      "Iteration 4530 : Loss 2975.0452\n",
      "Iteration 4540 : Loss 2974.8684\n",
      "Iteration 4550 : Loss 2974.6930\n",
      "Iteration 4560 : Loss 2974.5191\n",
      "Iteration 4570 : Loss 2974.3466\n",
      "Iteration 4580 : Loss 2974.1756\n",
      "Iteration 4590 : Loss 2974.0059\n",
      "Iteration 4600 : Loss 2973.8377\n",
      "Iteration 4610 : Loss 2973.6708\n",
      "Iteration 4620 : Loss 2973.5054\n",
      "Iteration 4630 : Loss 2973.3413\n",
      "Iteration 4640 : Loss 2973.1786\n",
      "Iteration 4650 : Loss 2973.0172\n",
      "Iteration 4660 : Loss 2972.8571\n",
      "Iteration 4670 : Loss 2972.6983\n",
      "Iteration 4680 : Loss 2972.5409\n",
      "Iteration 4690 : Loss 2972.3847\n",
      "Iteration 4700 : Loss 2972.2299\n",
      "Iteration 4710 : Loss 2972.0763\n",
      "Iteration 4720 : Loss 2971.9239\n",
      "Iteration 4730 : Loss 2971.7728\n",
      "Iteration 4740 : Loss 2971.6230\n",
      "Iteration 4750 : Loss 2971.4743\n",
      "Iteration 4760 : Loss 2971.3269\n",
      "Iteration 4770 : Loss 2971.1807\n",
      "Iteration 4780 : Loss 2971.0357\n",
      "Iteration 4790 : Loss 2970.8919\n",
      "Iteration 4800 : Loss 2970.7492\n",
      "Iteration 4810 : Loss 2970.6077\n",
      "Iteration 4820 : Loss 2970.4673\n",
      "Iteration 4830 : Loss 2970.3281\n",
      "Iteration 4840 : Loss 2970.1901\n",
      "Iteration 4850 : Loss 2970.0531\n",
      "Iteration 4860 : Loss 2969.9172\n",
      "Iteration 4870 : Loss 2969.7825\n",
      "Iteration 4880 : Loss 2969.6488\n",
      "Iteration 4890 : Loss 2969.5163\n",
      "Iteration 4900 : Loss 2969.3847\n",
      "Iteration 4910 : Loss 2969.2543\n",
      "Iteration 4920 : Loss 2969.1249\n",
      "Iteration 4930 : Loss 2968.9966\n",
      "Iteration 4940 : Loss 2968.8692\n",
      "Iteration 4950 : Loss 2968.7430\n",
      "Iteration 4960 : Loss 2968.6177\n",
      "Iteration 4970 : Loss 2968.4934\n",
      "Iteration 4980 : Loss 2968.3701\n",
      "Iteration 4990 : Loss 2968.2479\n",
      "Iteration 5000 : Loss 2968.1266\n",
      "Iteration 5010 : Loss 2968.0062\n",
      "Iteration 5020 : Loss 2967.8869\n",
      "Iteration 5030 : Loss 2967.7684\n",
      "Iteration 5040 : Loss 2967.6510\n",
      "Iteration 5050 : Loss 2967.5344\n",
      "Iteration 5060 : Loss 2967.4188\n",
      "Iteration 5070 : Loss 2967.3042\n",
      "Iteration 5080 : Loss 2967.1904\n",
      "Iteration 5090 : Loss 2967.0775\n",
      "Iteration 5100 : Loss 2966.9656\n",
      "Iteration 5110 : Loss 2966.8545\n",
      "Iteration 5120 : Loss 2966.7443\n",
      "Iteration 5130 : Loss 2966.6350\n",
      "Iteration 5140 : Loss 2966.5265\n",
      "Iteration 5150 : Loss 2966.4189\n",
      "Iteration 5160 : Loss 2966.3122\n",
      "Iteration 5170 : Loss 2966.2063\n",
      "Iteration 5180 : Loss 2966.1012\n",
      "Iteration 5190 : Loss 2965.9970\n",
      "Iteration 5200 : Loss 2965.8935\n",
      "Iteration 5210 : Loss 2965.7909\n",
      "Iteration 5220 : Loss 2965.6892\n",
      "Iteration 5230 : Loss 2965.5882\n",
      "Iteration 5240 : Loss 2965.4880\n",
      "Iteration 5250 : Loss 2965.3885\n",
      "Iteration 5260 : Loss 2965.2899\n",
      "Iteration 5270 : Loss 2965.1920\n",
      "Iteration 5280 : Loss 2965.0949\n",
      "Iteration 5290 : Loss 2964.9986\n",
      "Iteration 5300 : Loss 2964.9030\n",
      "Iteration 5310 : Loss 2964.8082\n",
      "Iteration 5320 : Loss 2964.7141\n",
      "Iteration 5330 : Loss 2964.6207\n",
      "Iteration 5340 : Loss 2964.5281\n",
      "Iteration 5350 : Loss 2964.4362\n",
      "Iteration 5360 : Loss 2964.3450\n",
      "Iteration 5370 : Loss 2964.2545\n",
      "Iteration 5380 : Loss 2964.1647\n",
      "Iteration 5390 : Loss 2964.0756\n",
      "Iteration 5400 : Loss 2963.9872\n",
      "Iteration 5410 : Loss 2963.8994\n",
      "Iteration 5420 : Loss 2963.8124\n",
      "Iteration 5430 : Loss 2963.7260\n",
      "Iteration 5440 : Loss 2963.6403\n",
      "Iteration 5450 : Loss 2963.5553\n",
      "Iteration 5460 : Loss 2963.4709\n",
      "Iteration 5470 : Loss 2963.3871\n",
      "Iteration 5480 : Loss 2963.3040\n",
      "Iteration 5490 : Loss 2963.2216\n",
      "Iteration 5500 : Loss 2963.1398\n",
      "Iteration 5510 : Loss 2963.0586\n",
      "Iteration 5520 : Loss 2962.9780\n",
      "Iteration 5530 : Loss 2962.8980\n",
      "Iteration 5540 : Loss 2962.8187\n",
      "Iteration 5550 : Loss 2962.7399\n",
      "Iteration 5560 : Loss 2962.6618\n",
      "Iteration 5570 : Loss 2962.5842\n",
      "Iteration 5580 : Loss 2962.5073\n",
      "Iteration 5590 : Loss 2962.4309\n",
      "Iteration 5600 : Loss 2962.3551\n",
      "Iteration 5610 : Loss 2962.2799\n",
      "Iteration 5620 : Loss 2962.2053\n",
      "Iteration 5630 : Loss 2962.1312\n",
      "Iteration 5640 : Loss 2962.0577\n",
      "Iteration 5650 : Loss 2961.9847\n",
      "Iteration 5660 : Loss 2961.9123\n",
      "Iteration 5670 : Loss 2961.8405\n",
      "Iteration 5680 : Loss 2961.7692\n",
      "Iteration 5690 : Loss 2961.6984\n",
      "Iteration 5700 : Loss 2961.6281\n",
      "Iteration 5710 : Loss 2961.5584\n",
      "Iteration 5720 : Loss 2961.4892\n",
      "Iteration 5730 : Loss 2961.4206\n",
      "Iteration 5740 : Loss 2961.3524\n",
      "Iteration 5750 : Loss 2961.2848\n",
      "Iteration 5760 : Loss 2961.2176\n",
      "Iteration 5770 : Loss 2961.1510\n",
      "Iteration 5780 : Loss 2961.0849\n",
      "Iteration 5790 : Loss 2961.0192\n",
      "Iteration 5800 : Loss 2960.9541\n",
      "Iteration 5810 : Loss 2960.8894\n",
      "Iteration 5820 : Loss 2960.8252\n",
      "Iteration 5830 : Loss 2960.7615\n",
      "Iteration 5840 : Loss 2960.6983\n",
      "Iteration 5850 : Loss 2960.6355\n",
      "Iteration 5860 : Loss 2960.5732\n",
      "Iteration 5870 : Loss 2960.5114\n",
      "Iteration 5880 : Loss 2960.4500\n",
      "Iteration 5890 : Loss 2960.3891\n",
      "Iteration 5900 : Loss 2960.3286\n",
      "Iteration 5910 : Loss 2960.2686\n",
      "Iteration 5920 : Loss 2960.2090\n",
      "Iteration 5930 : Loss 2960.1499\n",
      "Iteration 5940 : Loss 2960.0912\n",
      "Iteration 5950 : Loss 2960.0329\n",
      "Iteration 5960 : Loss 2959.9750\n",
      "Iteration 5970 : Loss 2959.9176\n",
      "Iteration 5980 : Loss 2959.8606\n",
      "Iteration 5990 : Loss 2959.8040\n",
      "Iteration 6000 : Loss 2959.7479\n",
      "Iteration 6010 : Loss 2959.6921\n",
      "Iteration 6020 : Loss 2959.6367\n",
      "Iteration 6030 : Loss 2959.5818\n",
      "Iteration 6040 : Loss 2959.5272\n",
      "Iteration 6050 : Loss 2959.4731\n",
      "Iteration 6060 : Loss 2959.4193\n",
      "Iteration 6070 : Loss 2959.3660\n",
      "Iteration 6080 : Loss 2959.3130\n",
      "Iteration 6090 : Loss 2959.2604\n",
      "Iteration 6100 : Loss 2959.2082\n",
      "Iteration 6110 : Loss 2959.1564\n",
      "Iteration 6120 : Loss 2959.1049\n",
      "Iteration 6130 : Loss 2959.0538\n",
      "Iteration 6140 : Loss 2959.0031\n",
      "Iteration 6150 : Loss 2958.9527\n",
      "Iteration 6160 : Loss 2958.9027\n",
      "Iteration 6170 : Loss 2958.8531\n",
      "Iteration 6180 : Loss 2958.8038\n",
      "Iteration 6190 : Loss 2958.7549\n",
      "Iteration 6200 : Loss 2958.7063\n",
      "Iteration 6210 : Loss 2958.6581\n",
      "Iteration 6220 : Loss 2958.6102\n",
      "Iteration 6230 : Loss 2958.5627\n",
      "Iteration 6240 : Loss 2958.5155\n",
      "Iteration 6250 : Loss 2958.4687\n",
      "Iteration 6260 : Loss 2958.4221\n",
      "Iteration 6270 : Loss 2958.3759\n",
      "Iteration 6280 : Loss 2958.3301\n",
      "Iteration 6290 : Loss 2958.2845\n",
      "Iteration 6300 : Loss 2958.2393\n",
      "Iteration 6310 : Loss 2958.1944\n",
      "Iteration 6320 : Loss 2958.1499\n",
      "Iteration 6330 : Loss 2958.1056\n",
      "Iteration 6340 : Loss 2958.0616\n",
      "Iteration 6350 : Loss 2958.0180\n",
      "Iteration 6360 : Loss 2957.9747\n",
      "Iteration 6370 : Loss 2957.9317\n",
      "Iteration 6380 : Loss 2957.8889\n",
      "Iteration 6390 : Loss 2957.8465\n",
      "Iteration 6400 : Loss 2957.8044\n",
      "Iteration 6410 : Loss 2957.7626\n",
      "Iteration 6420 : Loss 2957.7210\n",
      "Iteration 6430 : Loss 2957.6798\n",
      "Iteration 6440 : Loss 2957.6388\n",
      "Iteration 6450 : Loss 2957.5982\n",
      "Iteration 6460 : Loss 2957.5578\n",
      "Iteration 6470 : Loss 2957.5177\n",
      "Iteration 6480 : Loss 2957.4778\n",
      "Iteration 6490 : Loss 2957.4383\n",
      "Iteration 6500 : Loss 2957.3990\n",
      "Iteration 6510 : Loss 2957.3600\n",
      "Iteration 6520 : Loss 2957.3213\n",
      "Iteration 6530 : Loss 2957.2828\n",
      "Iteration 6540 : Loss 2957.2446\n",
      "Iteration 6550 : Loss 2957.2067\n",
      "Iteration 6560 : Loss 2957.1690\n",
      "Iteration 6570 : Loss 2957.1316\n",
      "Iteration 6580 : Loss 2957.0945\n",
      "Iteration 6590 : Loss 2957.0576\n",
      "Iteration 6600 : Loss 2957.0209\n",
      "Iteration 6610 : Loss 2956.9845\n",
      "Iteration 6620 : Loss 2956.9484\n",
      "Iteration 6630 : Loss 2956.9125\n",
      "Iteration 6640 : Loss 2956.8768\n",
      "Iteration 6650 : Loss 2956.8414\n",
      "Iteration 6660 : Loss 2956.8062\n",
      "Iteration 6670 : Loss 2956.7713\n",
      "Iteration 6680 : Loss 2956.7366\n",
      "Iteration 6690 : Loss 2956.7022\n",
      "Iteration 6700 : Loss 2956.6680\n",
      "Iteration 6710 : Loss 2956.6340\n",
      "Iteration 6720 : Loss 2956.6002\n",
      "Iteration 6730 : Loss 2956.5667\n",
      "Iteration 6740 : Loss 2956.5334\n",
      "Iteration 6750 : Loss 2956.5003\n",
      "Iteration 6760 : Loss 2956.4674\n",
      "Iteration 6770 : Loss 2956.4348\n",
      "Iteration 6780 : Loss 2956.4024\n",
      "Iteration 6790 : Loss 2956.3702\n",
      "Iteration 6800 : Loss 2956.3382\n",
      "Iteration 6810 : Loss 2956.3065\n",
      "Iteration 6820 : Loss 2956.2749\n",
      "Iteration 6830 : Loss 2956.2436\n",
      "Iteration 6840 : Loss 2956.2124\n",
      "Iteration 6850 : Loss 2956.1815\n",
      "Iteration 6860 : Loss 2956.1508\n",
      "Iteration 6870 : Loss 2956.1203\n",
      "Iteration 6880 : Loss 2956.0900\n",
      "Iteration 6890 : Loss 2956.0598\n",
      "Iteration 6900 : Loss 2956.0299\n",
      "Iteration 6910 : Loss 2956.0002\n",
      "Iteration 6920 : Loss 2955.9707\n",
      "Iteration 6930 : Loss 2955.9414\n",
      "Iteration 6940 : Loss 2955.9123\n",
      "Iteration 6950 : Loss 2955.8833\n",
      "Iteration 6960 : Loss 2955.8546\n",
      "Iteration 6970 : Loss 2955.8260\n",
      "Iteration 6980 : Loss 2955.7976\n",
      "Iteration 6990 : Loss 2955.7695\n",
      "Iteration 7000 : Loss 2955.7415\n",
      "Iteration 7010 : Loss 2955.7136\n",
      "Iteration 7020 : Loss 2955.6860\n",
      "Iteration 7030 : Loss 2955.6586\n",
      "Iteration 7040 : Loss 2955.6313\n",
      "Iteration 7050 : Loss 2955.6042\n",
      "Iteration 7060 : Loss 2955.5773\n",
      "Iteration 7070 : Loss 2955.5505\n",
      "Iteration 7080 : Loss 2955.5239\n",
      "Iteration 7090 : Loss 2955.4975\n",
      "Iteration 7100 : Loss 2955.4713\n",
      "Iteration 7110 : Loss 2955.4453\n",
      "Iteration 7120 : Loss 2955.4194\n",
      "Iteration 7130 : Loss 2955.3936\n",
      "Iteration 7140 : Loss 2955.3681\n",
      "Iteration 7150 : Loss 2955.3427\n",
      "Iteration 7160 : Loss 2955.3175\n",
      "Iteration 7170 : Loss 2955.2924\n",
      "Iteration 7180 : Loss 2955.2675\n",
      "Iteration 7190 : Loss 2955.2427\n",
      "Iteration 7200 : Loss 2955.2181\n",
      "Iteration 7210 : Loss 2955.1937\n",
      "Iteration 7220 : Loss 2955.1694\n",
      "Iteration 7230 : Loss 2955.1453\n",
      "Iteration 7240 : Loss 2955.1213\n",
      "Iteration 7250 : Loss 2955.0975\n",
      "Iteration 7260 : Loss 2955.0738\n",
      "Iteration 7270 : Loss 2955.0503\n",
      "Iteration 7280 : Loss 2955.0269\n",
      "Iteration 7290 : Loss 2955.0037\n",
      "Iteration 7300 : Loss 2954.9806\n",
      "Iteration 7310 : Loss 2954.9577\n",
      "Iteration 7320 : Loss 2954.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7330 : Loss 2954.9122\n",
      "Iteration 7340 : Loss 2954.8897\n",
      "Iteration 7350 : Loss 2954.8674\n",
      "Iteration 7360 : Loss 2954.8451\n",
      "Iteration 7370 : Loss 2954.8231\n",
      "Iteration 7380 : Loss 2954.8011\n",
      "Iteration 7390 : Loss 2954.7793\n",
      "Iteration 7400 : Loss 2954.7576\n",
      "Iteration 7410 : Loss 2954.7361\n",
      "Iteration 7420 : Loss 2954.7147\n",
      "Iteration 7430 : Loss 2954.6934\n",
      "Iteration 7440 : Loss 2954.6723\n",
      "Iteration 7450 : Loss 2954.6512\n",
      "Iteration 7460 : Loss 2954.6304\n",
      "Iteration 7470 : Loss 2954.6096\n",
      "Iteration 7480 : Loss 2954.5890\n",
      "Iteration 7490 : Loss 2954.5685\n",
      "Iteration 7500 : Loss 2954.5481\n",
      "Iteration 7510 : Loss 2954.5278\n",
      "Iteration 7520 : Loss 2954.5077\n",
      "Iteration 7530 : Loss 2954.4877\n",
      "Iteration 7540 : Loss 2954.4678\n",
      "Iteration 7550 : Loss 2954.4481\n",
      "Iteration 7560 : Loss 2954.4284\n",
      "Iteration 7570 : Loss 2954.4089\n",
      "Iteration 7580 : Loss 2954.3895\n",
      "Iteration 7590 : Loss 2954.3702\n",
      "Iteration 7600 : Loss 2954.3510\n",
      "Iteration 7610 : Loss 2954.3320\n",
      "Iteration 7620 : Loss 2954.3130\n",
      "Iteration 7630 : Loss 2954.2942\n",
      "Iteration 7640 : Loss 2954.2755\n",
      "Iteration 7650 : Loss 2954.2569\n",
      "Iteration 7660 : Loss 2954.2384\n",
      "Iteration 7670 : Loss 2954.2200\n",
      "Iteration 7680 : Loss 2954.2018\n",
      "Iteration 7690 : Loss 2954.1836\n",
      "Iteration 7700 : Loss 2954.1655\n",
      "Iteration 7710 : Loss 2954.1476\n",
      "Iteration 7720 : Loss 2954.1298\n",
      "Iteration 7730 : Loss 2954.1120\n",
      "Iteration 7740 : Loss 2954.0944\n",
      "Iteration 7750 : Loss 2954.0769\n",
      "Iteration 7760 : Loss 2954.0595\n",
      "Iteration 7770 : Loss 2954.0421\n",
      "Iteration 7780 : Loss 2954.0249\n",
      "Iteration 7790 : Loss 2954.0078\n",
      "Iteration 7800 : Loss 2953.9908\n",
      "Iteration 7810 : Loss 2953.9739\n",
      "Iteration 7820 : Loss 2953.9571\n",
      "Iteration 7830 : Loss 2953.9403\n",
      "Iteration 7840 : Loss 2953.9237\n",
      "Iteration 7850 : Loss 2953.9072\n",
      "Iteration 7860 : Loss 2953.8908\n",
      "Iteration 7870 : Loss 2953.8744\n",
      "Iteration 7880 : Loss 2953.8582\n",
      "Iteration 7890 : Loss 2953.8420\n",
      "Iteration 7900 : Loss 2953.8260\n",
      "Iteration 7910 : Loss 2953.8100\n",
      "Iteration 7920 : Loss 2953.7942\n",
      "Iteration 7930 : Loss 2953.7784\n",
      "Iteration 7940 : Loss 2953.7627\n",
      "Iteration 7950 : Loss 2953.7471\n",
      "Iteration 7960 : Loss 2953.7316\n",
      "Iteration 7970 : Loss 2953.7162\n",
      "Iteration 7980 : Loss 2953.7008\n",
      "Iteration 7990 : Loss 2953.6856\n",
      "Iteration 8000 : Loss 2953.6704\n",
      "Iteration 8010 : Loss 2953.6554\n",
      "Iteration 8020 : Loss 2953.6404\n",
      "Iteration 8030 : Loss 2953.6255\n",
      "Iteration 8040 : Loss 2953.6107\n",
      "Iteration 8050 : Loss 2953.5959\n",
      "Iteration 8060 : Loss 2953.5813\n",
      "Iteration 8070 : Loss 2953.5667\n",
      "Iteration 8080 : Loss 2953.5522\n",
      "Iteration 8090 : Loss 2953.5378\n",
      "Iteration 8100 : Loss 2953.5235\n",
      "Iteration 8110 : Loss 2953.5092\n",
      "Iteration 8120 : Loss 2953.4951\n",
      "Iteration 8130 : Loss 2953.4810\n",
      "Iteration 8140 : Loss 2953.4670\n",
      "Iteration 8150 : Loss 2953.4530\n",
      "Iteration 8160 : Loss 2953.4392\n",
      "Iteration 8170 : Loss 2953.4254\n",
      "Iteration 8180 : Loss 2953.4117\n",
      "Iteration 8190 : Loss 2953.3981\n",
      "Iteration 8200 : Loss 2953.3845\n",
      "Iteration 8210 : Loss 2953.3710\n",
      "Iteration 8220 : Loss 2953.3576\n",
      "Iteration 8230 : Loss 2953.3443\n",
      "Iteration 8240 : Loss 2953.3310\n",
      "Iteration 8250 : Loss 2953.3179\n",
      "Iteration 8260 : Loss 2953.3047\n",
      "Iteration 8270 : Loss 2953.2917\n",
      "Iteration 8280 : Loss 2953.2787\n",
      "Iteration 8290 : Loss 2953.2658\n",
      "Iteration 8300 : Loss 2953.2530\n",
      "Iteration 8310 : Loss 2953.2402\n",
      "Iteration 8320 : Loss 2953.2275\n",
      "Iteration 8330 : Loss 2953.2149\n",
      "Iteration 8340 : Loss 2953.2023\n",
      "Iteration 8350 : Loss 2953.1898\n",
      "Iteration 8360 : Loss 2953.1774\n",
      "Iteration 8370 : Loss 2953.1651\n",
      "Iteration 8380 : Loss 2953.1528\n",
      "Iteration 8390 : Loss 2953.1405\n",
      "Iteration 8400 : Loss 2953.1284\n",
      "Iteration 8410 : Loss 2953.1163\n",
      "Iteration 8420 : Loss 2953.1042\n",
      "Iteration 8430 : Loss 2953.0923\n",
      "Iteration 8440 : Loss 2953.0803\n",
      "Iteration 8450 : Loss 2953.0685\n",
      "Iteration 8460 : Loss 2953.0567\n",
      "Iteration 8470 : Loss 2953.0450\n",
      "Iteration 8480 : Loss 2953.0333\n",
      "Iteration 8490 : Loss 2953.0217\n",
      "Iteration 8500 : Loss 2953.0102\n",
      "Iteration 8510 : Loss 2952.9987\n",
      "Iteration 8520 : Loss 2952.9873\n",
      "Iteration 8530 : Loss 2952.9759\n",
      "Iteration 8540 : Loss 2952.9646\n",
      "Iteration 8550 : Loss 2952.9534\n",
      "Iteration 8560 : Loss 2952.9422\n",
      "Iteration 8570 : Loss 2952.9310\n",
      "Iteration 8580 : Loss 2952.9200\n",
      "Iteration 8590 : Loss 2952.9090\n",
      "Iteration 8600 : Loss 2952.8980\n",
      "Iteration 8610 : Loss 2952.8871\n",
      "Iteration 8620 : Loss 2952.8762\n",
      "Iteration 8630 : Loss 2952.8654\n",
      "Iteration 8640 : Loss 2952.8547\n",
      "Iteration 8650 : Loss 2952.8440\n",
      "Iteration 8660 : Loss 2952.8334\n",
      "Iteration 8670 : Loss 2952.8228\n",
      "Iteration 8680 : Loss 2952.8123\n",
      "Iteration 8690 : Loss 2952.8018\n",
      "Iteration 8700 : Loss 2952.7914\n",
      "Iteration 8710 : Loss 2952.7810\n",
      "Iteration 8720 : Loss 2952.7707\n",
      "Iteration 8730 : Loss 2952.7604\n",
      "Iteration 8740 : Loss 2952.7502\n",
      "Iteration 8750 : Loss 2952.7401\n",
      "Iteration 8760 : Loss 2952.7299\n",
      "Iteration 8770 : Loss 2952.7199\n",
      "Iteration 8780 : Loss 2952.7099\n",
      "Iteration 8790 : Loss 2952.6999\n",
      "Iteration 8800 : Loss 2952.6900\n",
      "Iteration 8810 : Loss 2952.6801\n",
      "Iteration 8820 : Loss 2952.6703\n",
      "Iteration 8830 : Loss 2952.6605\n",
      "Iteration 8840 : Loss 2952.6508\n",
      "Iteration 8850 : Loss 2952.6411\n",
      "Iteration 8860 : Loss 2952.6315\n",
      "Iteration 8870 : Loss 2952.6219\n",
      "Iteration 8880 : Loss 2952.6124\n",
      "Iteration 8890 : Loss 2952.6029\n",
      "Iteration 8900 : Loss 2952.5934\n",
      "Iteration 8910 : Loss 2952.5840\n",
      "Iteration 8920 : Loss 2952.5747\n",
      "Iteration 8930 : Loss 2952.5654\n",
      "Iteration 8940 : Loss 2952.5561\n",
      "Iteration 8950 : Loss 2952.5469\n",
      "Iteration 8960 : Loss 2952.5377\n",
      "Iteration 8970 : Loss 2952.5286\n",
      "Iteration 8980 : Loss 2952.5195\n",
      "Iteration 8990 : Loss 2952.5104\n",
      "Iteration 9000 : Loss 2952.5014\n",
      "Iteration 9010 : Loss 2952.4925\n",
      "Iteration 9020 : Loss 2952.4836\n",
      "Iteration 9030 : Loss 2952.4747\n",
      "Iteration 9040 : Loss 2952.4658\n",
      "Iteration 9050 : Loss 2952.4570\n",
      "Iteration 9060 : Loss 2952.4483\n",
      "Iteration 9070 : Loss 2952.4396\n",
      "Iteration 9080 : Loss 2952.4309\n",
      "Iteration 9090 : Loss 2952.4222\n",
      "Iteration 9100 : Loss 2952.4137\n",
      "Iteration 9110 : Loss 2952.4051\n",
      "Iteration 9120 : Loss 2952.3966\n",
      "Iteration 9130 : Loss 2952.3881\n",
      "Iteration 9140 : Loss 2952.3797\n",
      "Iteration 9150 : Loss 2952.3713\n",
      "Iteration 9160 : Loss 2952.3629\n",
      "Iteration 9170 : Loss 2952.3546\n",
      "Iteration 9180 : Loss 2952.3463\n",
      "Iteration 9190 : Loss 2952.3380\n",
      "Iteration 9200 : Loss 2952.3298\n",
      "Iteration 9210 : Loss 2952.3216\n",
      "Iteration 9220 : Loss 2952.3135\n",
      "Iteration 9230 : Loss 2952.3054\n",
      "Iteration 9240 : Loss 2952.2973\n",
      "Iteration 9250 : Loss 2952.2893\n",
      "Iteration 9260 : Loss 2952.2813\n",
      "Iteration 9270 : Loss 2952.2733\n",
      "Iteration 9280 : Loss 2952.2654\n",
      "Iteration 9290 : Loss 2952.2575\n",
      "Iteration 9300 : Loss 2952.2497\n",
      "Iteration 9310 : Loss 2952.2418\n",
      "Iteration 9320 : Loss 2952.2341\n",
      "Iteration 9330 : Loss 2952.2263\n",
      "Iteration 9340 : Loss 2952.2186\n",
      "Iteration 9350 : Loss 2952.2109\n",
      "Iteration 9360 : Loss 2952.2032\n",
      "Iteration 9370 : Loss 2952.1956\n",
      "Iteration 9380 : Loss 2952.1880\n",
      "Iteration 9390 : Loss 2952.1805\n",
      "Iteration 9400 : Loss 2952.1730\n",
      "Iteration 9410 : Loss 2952.1655\n",
      "Iteration 9420 : Loss 2952.1580\n",
      "Iteration 9430 : Loss 2952.1506\n",
      "Iteration 9440 : Loss 2952.1432\n",
      "Iteration 9450 : Loss 2952.1358\n",
      "Iteration 9460 : Loss 2952.1285\n",
      "Iteration 9470 : Loss 2952.1212\n",
      "Iteration 9480 : Loss 2952.1139\n",
      "Iteration 9490 : Loss 2952.1067\n",
      "Iteration 9500 : Loss 2952.0995\n",
      "Iteration 9510 : Loss 2952.0923\n",
      "Iteration 9520 : Loss 2952.0852\n",
      "Iteration 9530 : Loss 2952.0780\n",
      "Iteration 9540 : Loss 2952.0709\n",
      "Iteration 9550 : Loss 2952.0639\n",
      "Iteration 9560 : Loss 2952.0569\n",
      "Iteration 9570 : Loss 2952.0499\n",
      "Iteration 9580 : Loss 2952.0429\n",
      "Iteration 9590 : Loss 2952.0359\n",
      "Iteration 9600 : Loss 2952.0290\n",
      "Iteration 9610 : Loss 2952.0221\n",
      "Iteration 9620 : Loss 2952.0153\n",
      "Iteration 9630 : Loss 2952.0084\n",
      "Iteration 9640 : Loss 2952.0016\n",
      "Iteration 9650 : Loss 2951.9949\n",
      "Iteration 9660 : Loss 2951.9881\n",
      "Iteration 9670 : Loss 2951.9814\n",
      "Iteration 9680 : Loss 2951.9747\n",
      "Iteration 9690 : Loss 2951.9680\n",
      "Iteration 9700 : Loss 2951.9614\n",
      "Iteration 9710 : Loss 2951.9548\n",
      "Iteration 9720 : Loss 2951.9482\n",
      "Iteration 9730 : Loss 2951.9416\n",
      "Iteration 9740 : Loss 2951.9351\n",
      "Iteration 9750 : Loss 2951.9286\n",
      "Iteration 9760 : Loss 2951.9221\n",
      "Iteration 9770 : Loss 2951.9156\n",
      "Iteration 9780 : Loss 2951.9092\n",
      "Iteration 9790 : Loss 2951.9028\n",
      "Iteration 9800 : Loss 2951.8964\n",
      "Iteration 9810 : Loss 2951.8900\n",
      "Iteration 9820 : Loss 2951.8837\n",
      "Iteration 9830 : Loss 2951.8774\n",
      "Iteration 9840 : Loss 2951.8711\n",
      "Iteration 9850 : Loss 2951.8648\n",
      "Iteration 9860 : Loss 2951.8586\n",
      "Iteration 9870 : Loss 2951.8523\n",
      "Iteration 9880 : Loss 2951.8462\n",
      "Iteration 9890 : Loss 2951.8400\n",
      "Iteration 9900 : Loss 2951.8338\n",
      "Iteration 9910 : Loss 2951.8277\n",
      "Iteration 9920 : Loss 2951.8216\n",
      "Iteration 9930 : Loss 2951.8155\n",
      "Iteration 9940 : Loss 2951.8095\n",
      "Iteration 9950 : Loss 2951.8035\n",
      "Iteration 9960 : Loss 2951.7974\n",
      "Iteration 9970 : Loss 2951.7915\n",
      "Iteration 9980 : Loss 2951.7855\n",
      "Iteration 9990 : Loss 2951.7795\n",
      "Iteration 10000 : Loss 2951.7736\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ae4a3",
   "metadata": {},
   "source": [
    "## (10) test 데이터에 대한 성능 확인하기\n",
    "\n",
    "test 데이터에 대한 성능을 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a8133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2824.4317785960575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acaebf",
   "metadata": {},
   "source": [
    "## (11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "\n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4aeb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"true\")\n",
    "plt.scatter(X_test[:, 0], prediction, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
